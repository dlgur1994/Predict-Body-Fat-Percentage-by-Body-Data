{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/anaconda3/lib/python3.8/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.8/site-packages (from xgboost) (1.18.5)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.8/site-packages (from xgboost) (1.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Homebrew...\n",
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updated Homebrew!\u001b[0m\n",
      "Updated 2 taps (homebrew/core and homebrew/services).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Formulae\u001b[0m\n",
      "argocd-autopilot    erlang@23           maturin             qthreads\n",
      "at-spi2-atk         gcc@10              moar                range2cidr\n",
      "at-spi2-core        gitbackup           mongocli            rmw\n",
      "atuin               gitwatch            mongosh             scotch\n",
      "autoconf@2.69       gpg-tui             neovim-remote       search-that-hash\n",
      "avahi               gradle@6            nomino              simde\n",
      "cadical             grepip              openexr@2           slides\n",
      "caire               himalaya            opensearch          sqlbench\n",
      "cidr2range          ipinfo-cli          osinfo-db           sqlx-cli\n",
      "clazy               julia               osinfo-db-tools     storj-uplink\n",
      "code-minimap        libmobi             parquet-cli         tbb@2020\n",
      "ddcctl              lsix                php-cs-fixer@2      tmuxp\n",
      "ehco                marp-cli            principalmapper     trojan-go\n",
      "elan-init           mathlibtools        proj@7              universal-ctags\n",
      "elfutils            matterbridge        pywhat              zellij\n",
      "\u001b[34m==>\u001b[0m \u001b[1mUpdated Formulae\u001b[0m\n",
      "Updated 1510 formulae.\n",
      "\u001b[34m==>\u001b[0m \u001b[1mRenamed Formulae\u001b[0m\n",
      "badtouch -> authoscope                   grakn -> typedb\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDeleted Formulae\u001b[0m\n",
      "aurora-cli      erlang@20       osquery         protobuf-swift  protobuf@3.7\n",
      "\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libomp/manifests/12.0.0\u001b[0m\n",
      "######################################################################## 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libomp/blobs/sha256:e6ccdea1356\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sh\u001b[0m\n",
      "######################################################################## 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring libomp--12.0.0.mojave.bottle.tar.gz\u001b[0m\n",
      "🍺  /usr/local/Cellar/libomp/12.0.0: 9 files, 1.5MB\n",
      "\u001b[34m==>\u001b[0m \u001b[1m`brew cleanup` has not been run in 30 days, running now...\u001b[0m\n",
      "Removing: /usr/local/Cellar/gdbm/1.18.1... (20 files, 586.9KB)\n",
      "Removing: /usr/local/Cellar/libuv/1.31.0... (48 files, 2.9MB)\n",
      "Removing: /Users/m/Library/Caches/Homebrew/pyenv--1.2.27... (628.9KB)\n",
      "Removing: /Users/m/Library/Caches/Homebrew/python@3.9--3.9.4... (13.5MB)\n",
      "Removing: /Users/m/Library/Logs/Homebrew/autoconf... (68B)\n",
      "Removing: /Users/m/Library/Logs/Homebrew/cjson... (68B)\n",
      "Removing: /Users/m/Library/Logs/Homebrew/gdbm... (68B)\n",
      "Removing: /Users/m/Library/Logs/Homebrew/libevent... (68B)\n",
      "Removing: /Users/m/Library/Logs/Homebrew/libuv... (68B)\n",
      "Removing: /Users/m/Library/Logs/Homebrew/libwebsockets... (68B)\n",
      "Removing: /Users/m/Library/Logs/Homebrew/m4... (68B)\n",
      "Removing: /Users/m/Library/Logs/Homebrew/mosquitto... (68B)\n",
      "Removing: /Users/m/Library/Logs/Homebrew/mpdecimal... (68B)\n",
      "Removing: /Users/m/Library/Logs/Homebrew/openssl@1.1... (68B)\n",
      "Removing: /Users/m/Library/Logs/Homebrew/pkg-config... (68B)\n",
      "Removing: /Users/m/Library/Logs/Homebrew/pyenv... (68B)\n",
      "Removing: /Users/m/Library/Logs/Homebrew/python@3.9... (2 files, 4.5KB)\n",
      "Removing: /Users/m/Library/Logs/Homebrew/readline... (68B)\n",
      "Removing: /Users/m/Library/Logs/Homebrew/sqlite... (68B)\n",
      "Pruned 3 symbolic links and 2 directories from /usr/local\n"
     ]
    }
   ],
   "source": [
    "!brew install libomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.2.1-py3-none-macosx_10_14_x86_64.macosx_10_15_x86_64.macosx_11_0_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 1.6 MB/s eta 0:00:01     |█████████████████               | 645 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/anaconda3/lib/python3.8/site-packages (from lightgbm) (1.5.0)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /opt/anaconda3/lib/python3.8/site-packages (from lightgbm) (0.23.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.8/site-packages (from lightgbm) (1.18.5)\n",
      "Requirement already satisfied: wheel in /opt/anaconda3/lib/python3.8/site-packages (from lightgbm) (0.34.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### LinearRegression ###\n",
      "MAE: 2.902, MSE: 3.731, RMSE: 13.920, RMSLE: 2.902, R2: 0.158\n",
      "\n",
      "### Ridge ###\n",
      "MAE: 2.889, MSE: 3.711, RMSE: 13.771, RMSLE: 2.889, R2: 0.159\n",
      "\n",
      "### Lasso ###\n",
      "MAE: 3.818, MSE: 4.857, RMSE: 23.587, RMSLE: 3.818, R2: 0.212\n",
      "\n",
      "### ElasticNet ###\n",
      "MAE: 3.754, MSE: 4.786, RMSE: 22.904, RMSLE: 3.754, R2: 0.209\n",
      "\n",
      "### DecisionTreeRegressor ###\n",
      "MAE: 2.350, MSE: 3.346, RMSE: 11.195, RMSLE: 2.350, R2: 0.141\n",
      "\n",
      "### RandomForestRegressor ###\n",
      "MAE: 2.044, MSE: 2.876, RMSE: 8.273, RMSLE: 2.044, R2: 0.117\n",
      "\n",
      "### GradientBoostingRegressor ###\n",
      "MAE: 2.345, MSE: 3.108, RMSE: 9.662, RMSLE: 2.345, R2: 0.136\n",
      "\n",
      "### XGBRegressor ###\n",
      "MAE: 2.329, MSE: 3.095, RMSE: 9.581, RMSLE: 2.329, R2: 0.130\n",
      "\n",
      "### LGBMRegressor ###\n",
      "MAE: 2.578, MSE: 3.388, RMSE: 11.477, RMSLE: 2.578, R2: 0.141\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression , Ridge , Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def get_best_params(model, params):\n",
    "    grid_model = GridSearchCV(model, param_grid=params, scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_model.fit(X_features, y_target)\n",
    "    rmse = np.sqrt(-1* grid_model.best_score_)\n",
    "    print('{0} 5 CV best average RMSE value: {1}, best alpha:{2}'.format(model.__class__.__name__,np.round(rmse, 4), grid_model.best_params_))\n",
    "    return grid_model.best_estimator_\n",
    "\n",
    "def get_model_predict(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    # scale back by expm1() because the predicted result is predicted by log-translated data\n",
    "    y_test = np.expm1(y_test)\n",
    "    pred = np.expm1(pred)\n",
    "    print('\\n###',model.__class__.__name__,'###')\n",
    "    evaluate_regr(y_test, pred)\n",
    "\n",
    "# calculate MAE, MSE, RMSE, RMSLE\n",
    "def evaluate_regr(y,pred):\n",
    "    mae_val = mean_absolute_error(y,pred)\n",
    "    mse_val = mean_squared_error(y,pred)\n",
    "    rmse_val = rmse(y,pred)\n",
    "    rmsle_val = rmsle(y,pred)\n",
    "    r2_val = r2_score(y, pred)\n",
    "    print('MAE: {0:.3F}, MSE: {2:.3F}, RMSE: {1:.3F}, RMSLE: {0:.3F}, R2: {3:.3F}'.format(mae_val, mse_val, rmse_val, rmsle_val, r2_val))\n",
    "\n",
    "# calculate RMSLE using log1p(), not log() because of the NaN issue \n",
    "def rmsle(y, pred):\n",
    "    log_y = np.log1p(y)\n",
    "    log_pred = np.log1p(pred)\n",
    "    squared_error = (log_y - log_pred) ** 2\n",
    "    rmsle = np.sqrt(np.mean(squared_error))\n",
    "    return rmsle\n",
    "\n",
    "# calculate RMSE using mean_square_error() of Scikit-learn \n",
    "def rmse(y,pred):\n",
    "    return np.sqrt(mean_squared_error(y,pred))\n",
    "\n",
    "def print_coefficient(models):\n",
    "    for model in models:\n",
    "        print('\\n###',model.__class__.__name__,'###')\n",
    "        coeff = pd.Series(data=np.round(model.coef_, 3), index=X_features_ohe.columns )\n",
    "        print(coeff.sort_values(ascending=False))\n",
    "\n",
    "# visualize coefficients of features for linear regression models\n",
    "def visualize_coefficient(models):\n",
    "    fig, axs = plt.subplots(figsize=(24,10),nrows=1, ncols=4)\n",
    "    fig.tight_layout()\n",
    "    for i_num, model in enumerate(models):\n",
    "        coef_high, coef_low = get_top_bottom_coef(model)\n",
    "        coef_concat = pd.concat( [coef_high , coef_low] )\n",
    "        axs[i_num].set_title(model.__class__.__name__+' Coeffiecents', size=25)\n",
    "        axs[i_num].tick_params(axis=\"y\",direction=\"in\", pad=-120)\n",
    "        for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()):\n",
    "            label.set_fontsize(22)\n",
    "        sns.barplot(x=coef_concat.values, y=coef_concat.index , ax=axs[i_num])\n",
    "\n",
    "# extract top 3 of features, bottom 3 efficient for linear regression models\n",
    "def get_top_bottom_coef(model):\n",
    "    # create Series objects based on coef_ property\n",
    "    coef = pd.Series(model.coef_, index=X_features_ohe.columns)\n",
    "    coef_high = coef.sort_values(ascending=False).head(3)\n",
    "    coef_low = coef.sort_values(ascending=False).tail(3)\n",
    "    return coef_high, coef_low\n",
    "\n",
    "# visualize coefficients of features for regression tree models \n",
    "def visualize_ftr_importances(models):\n",
    "    fig, axs = plt.subplots(figsize=(24,10),nrows=1, ncols=5)\n",
    "    fig.tight_layout()\n",
    "    for i_num, model in enumerate(models):\n",
    "        ftr_top6 = get_top_features(model)\n",
    "        axs[i_num].set_title(model.__class__.__name__+' Feature Importances', size=17)\n",
    "        for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()):\n",
    "            label.set_fontsize(22)\n",
    "        sns.barplot(x=ftr_top6.values, y=ftr_top6.index , ax=axs[i_num])\n",
    "\n",
    "# extract top 3 of features, bottom 3 efficient for regression tree models\n",
    "def get_top_features(model):\n",
    "    ftr_importances_values = model.feature_importances_\n",
    "    ftr_importances = pd.Series(ftr_importances_values, index=X_features_ohe.columns)\n",
    "    ftr_top6 = ftr_importances.sort_values(ascending=False)[:6]\n",
    "    return ftr_top6\n",
    "\n",
    "# file load\n",
    "# delete 'Index' because it is provided when converted to a data frame, and delete 'Standard_Weight' because it is determined by the hegith\n",
    "file_df = pd.read_csv('./train_data.csv')\n",
    "target_name = 'Body_Fat_Rate'\n",
    "no_need_features = ['Index', 'Standard_Weight']\n",
    "category_features = ['Sex']\n",
    "\n",
    "# arrange X and y\n",
    "file_df.drop(no_need_features, axis=1, inplace=True)\n",
    "y_target = file_df[target_name]\n",
    "X_features = file_df.drop([target_name],axis=1,inplace=False)\n",
    "\n",
    "# visualize data to find outliers\n",
    "# for feature in X_features.drop(category_features, axis=1, inplace=False):\n",
    "#     plt.scatter(x = file_df[feature], y = y_target)\n",
    "#     plt.ylabel(target_name, fontsize=15)\n",
    "#     plt.xlabel(feature, fontsize=15)\n",
    "#     plt.show()\n",
    "\n",
    "# remove outlier\n",
    "outlier_name = 'Height'\n",
    "cond1 = file_df[outlier_name] < 60\n",
    "cond2 = file_df[target_name] < 30\n",
    "outlier_index = X_features[cond1 & cond2].index\n",
    "# print('Outlier index :', outlier_index.values)\n",
    "# print('X_feature shape before Outlier is removed:', X_features.shape)\n",
    "X_features.drop(outlier_index , axis=0, inplace=True)\n",
    "y_target.drop(outlier_index, axis=0, inplace=True)\n",
    "# print('X_feature shape after Outlier is removed:', X_features.shape)\n",
    "\n",
    "# figure out the extent of distortion in features --> if the degree of distortion is high(>1 or <-1), log transformation is performed.\n",
    "# 'Height' needs the log transformation\n",
    "features_index = file_df.drop(category_features, axis=1, inplace=False).dtypes.index\n",
    "skew_features = file_df[features_index].apply(lambda x : skew(x))\n",
    "# print(skew_features.sort_values(ascending=False))\n",
    "skew_features_change = skew_features[skew_features < -1]\n",
    "file_df[skew_features_change.index] = np.log1p(file_df[skew_features_change.index])\n",
    "\n",
    "# change the category feature to One-Hot Encoding --> 'Sex'\n",
    "X_features_ohe = pd.get_dummies(X_features, columns=category_features)\n",
    "# print(X_features_ohe)\n",
    "\n",
    "# the log transformation is applied on the target column to form a normal distribution\n",
    "y_target_log = np.log1p(y_target)\n",
    "# print(y_target)\n",
    "# print(y_target_log)\n",
    "\n",
    "# split train/test data based on feature dataset with One-Hot encoding\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features_ohe, y_target_log, test_size=0.2, random_state=0)\n",
    "\n",
    "# define the model\n",
    "lr_reg = LinearRegression()\n",
    "ridge_reg = Ridge(alpha=8)\n",
    "lasso_reg = Lasso(alpha=0.05)\n",
    "en_reg = ElasticNet(alpha=0.07)\n",
    "dt_reg = DecisionTreeRegressor(max_depth=7)\n",
    "rf_reg = RandomForestRegressor(max_depth=14, min_samples_leaf=2, min_samples_split=2, n_estimators=700, n_jobs=-1)\n",
    "gbm_reg = GradientBoostingRegressor(n_estimators=500, learning_rate=0.02, subsample=0.05)\n",
    "xgb_reg = XGBRegressor(n_estimators=120, eta=0.1, min_child_weight=3, max_depth=3)\n",
    "lgbm_reg = LGBMRegressor(n_estimators=1000, learning_rate=0.03, max_depth=3, min_child_samples=10, num_leaves=3)\n",
    "\n",
    "# find best parameters\n",
    "# ridge_params = { 'alpha':[0.01, 0.05, 0.09, 0.1, 0.11, 0.12, 0.5, 1, 3, 5, 8, 10, 12, 15, 20, 30, 40, 50]}\n",
    "# lasso_params = { 'alpha':[0.01, 0.05, 0.09, 0.1, 0.11, 0.12, 0.5, 1, 3, 5, 8, 10, 12, 15, 20, 30, 40, 50]}\n",
    "# en_params = { 'alpha':[0.07, 0.1, 0.5, 1, 3]}\n",
    "# dt_params = {'max_depth':[1,3,5,7,9]}\n",
    "# rf_params = {'n_estimators':[700], 'max_depth' : [14], 'min_samples_leaf' : [2], 'min_samples_split' : [2]}\n",
    "# gbm_params = {'learning_rate': [0.02], 'n_estimators':[500], 'subsample': [0.05]}\n",
    "# xgb_params = {'colsample_bytree': [1], 'eta': [0.1], 'max_depth': [3], 'min_child_weight': [3], 'n_estimators':[120]}\n",
    "# lgbm_params = {'learning_rate': [0.03], 'max_depth': [3], 'min_child_samples': [10], 'n_estimators':[1000], 'num_leaves': [3]}\n",
    "# best_rige = get_best_params(ridge_reg, ridge_params)\n",
    "# best_lasso = get_best_params(lasso_reg, lasso_params)\n",
    "# best_en = get_best_params(en_reg, en_params)\n",
    "# best_dt = get_best_params(dt_reg, dt_params)\n",
    "# best_rf = get_best_params(rf_reg, rf_params)\n",
    "# best_gbm = get_best_params(gbm_reg, gbm_params)\n",
    "# best_xgb = get_best_params(xgb_reg, xgb_params)\n",
    "# best_lgbm = get_best_params(lgbm_reg, lgbm_params)\n",
    "\n",
    "# linear regression models\n",
    "models_linear = [lr_reg, ridge_reg, lasso_reg, en_reg]\n",
    "for model in models_linear:\n",
    "    get_model_predict(model,X_train, X_test, y_train, y_test)\n",
    "\n",
    "# visualize coefficients of linear regression models\n",
    "# print_coefficient(models_linear)\n",
    "# visualize_coefficient(models_linear)\n",
    "\n",
    "# regression tree models\n",
    "models_tree = [dt_reg, rf_reg, gbm_reg, xgb_reg, lgbm_reg]\n",
    "for model in models_tree:\n",
    "    get_model_predict(model,X_train, X_test, y_train, y_test)\n",
    "\n",
    "# visualize coefficients of regression tree models\n",
    "# visualize_ftr_importances(models_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
